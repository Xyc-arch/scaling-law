{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 100.0,
  "eval_steps": 500,
  "global_step": 106900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.4677268475210477,
      "grad_norm": 3.788468599319458,
      "learning_rate": 4.976613657623948e-05,
      "loss": 0.8492,
      "step": 500
    },
    {
      "epoch": 0.9354536950420954,
      "grad_norm": 2.6841042041778564,
      "learning_rate": 4.9532273152478954e-05,
      "loss": 0.7323,
      "step": 1000
    },
    {
      "epoch": 1.4031805425631432,
      "grad_norm": 3.139767646789551,
      "learning_rate": 4.929840972871843e-05,
      "loss": 0.7142,
      "step": 1500
    },
    {
      "epoch": 1.8709073900841908,
      "grad_norm": 2.705376625061035,
      "learning_rate": 4.9064546304957906e-05,
      "loss": 0.7052,
      "step": 2000
    },
    {
      "epoch": 2.3386342376052385,
      "grad_norm": 1.8583831787109375,
      "learning_rate": 4.883068288119738e-05,
      "loss": 0.6991,
      "step": 2500
    },
    {
      "epoch": 2.8063610851262863,
      "grad_norm": 2.8615386486053467,
      "learning_rate": 4.859681945743686e-05,
      "loss": 0.6962,
      "step": 3000
    },
    {
      "epoch": 3.2740879326473338,
      "grad_norm": 1.8563650846481323,
      "learning_rate": 4.836295603367634e-05,
      "loss": 0.6928,
      "step": 3500
    },
    {
      "epoch": 3.7418147801683816,
      "grad_norm": 1.6308566331863403,
      "learning_rate": 4.812909260991581e-05,
      "loss": 0.6902,
      "step": 4000
    },
    {
      "epoch": 4.209541627689429,
      "grad_norm": 1.2780678272247314,
      "learning_rate": 4.789522918615529e-05,
      "loss": 0.6886,
      "step": 4500
    },
    {
      "epoch": 4.677268475210477,
      "grad_norm": 2.114955186843872,
      "learning_rate": 4.766136576239477e-05,
      "loss": 0.6863,
      "step": 5000
    },
    {
      "epoch": 5.144995322731525,
      "grad_norm": 1.5075693130493164,
      "learning_rate": 4.7427502338634236e-05,
      "loss": 0.6846,
      "step": 5500
    },
    {
      "epoch": 5.612722170252573,
      "grad_norm": 1.6529089212417603,
      "learning_rate": 4.719363891487372e-05,
      "loss": 0.6827,
      "step": 6000
    },
    {
      "epoch": 6.080449017773621,
      "grad_norm": 1.5696028470993042,
      "learning_rate": 4.6959775491113195e-05,
      "loss": 0.6816,
      "step": 6500
    },
    {
      "epoch": 6.5481758652946676,
      "grad_norm": 1.4855520725250244,
      "learning_rate": 4.672591206735267e-05,
      "loss": 0.6789,
      "step": 7000
    },
    {
      "epoch": 7.015902712815715,
      "grad_norm": 1.837728500366211,
      "learning_rate": 4.6492048643592146e-05,
      "loss": 0.6789,
      "step": 7500
    },
    {
      "epoch": 7.483629560336763,
      "grad_norm": 1.3158947229385376,
      "learning_rate": 4.6258185219831615e-05,
      "loss": 0.6763,
      "step": 8000
    },
    {
      "epoch": 7.951356407857811,
      "grad_norm": 1.005029320716858,
      "learning_rate": 4.60243217960711e-05,
      "loss": 0.6763,
      "step": 8500
    },
    {
      "epoch": 8.419083255378858,
      "grad_norm": 1.1057227849960327,
      "learning_rate": 4.5790458372310574e-05,
      "loss": 0.6737,
      "step": 9000
    },
    {
      "epoch": 8.886810102899906,
      "grad_norm": 1.7725770473480225,
      "learning_rate": 4.555659494855005e-05,
      "loss": 0.6742,
      "step": 9500
    },
    {
      "epoch": 9.354536950420954,
      "grad_norm": 1.391596794128418,
      "learning_rate": 4.5322731524789526e-05,
      "loss": 0.67,
      "step": 10000
    },
    {
      "epoch": 9.822263797942002,
      "grad_norm": 1.033008098602295,
      "learning_rate": 4.5088868101029e-05,
      "loss": 0.671,
      "step": 10500
    },
    {
      "epoch": 10.28999064546305,
      "grad_norm": 1.3355512619018555,
      "learning_rate": 4.485500467726848e-05,
      "loss": 0.6693,
      "step": 11000
    },
    {
      "epoch": 10.757717492984098,
      "grad_norm": 1.1942977905273438,
      "learning_rate": 4.462114125350795e-05,
      "loss": 0.6685,
      "step": 11500
    },
    {
      "epoch": 11.225444340505145,
      "grad_norm": 1.507218599319458,
      "learning_rate": 4.438727782974743e-05,
      "loss": 0.6677,
      "step": 12000
    },
    {
      "epoch": 11.693171188026193,
      "grad_norm": 1.2184553146362305,
      "learning_rate": 4.4153414405986905e-05,
      "loss": 0.6654,
      "step": 12500
    },
    {
      "epoch": 12.160898035547241,
      "grad_norm": 0.7451967000961304,
      "learning_rate": 4.391955098222638e-05,
      "loss": 0.6655,
      "step": 13000
    },
    {
      "epoch": 12.628624883068287,
      "grad_norm": 1.235879898071289,
      "learning_rate": 4.368568755846586e-05,
      "loss": 0.6631,
      "step": 13500
    },
    {
      "epoch": 13.096351730589335,
      "grad_norm": 1.0919057130813599,
      "learning_rate": 4.345182413470533e-05,
      "loss": 0.6639,
      "step": 14000
    },
    {
      "epoch": 13.564078578110383,
      "grad_norm": 0.815996527671814,
      "learning_rate": 4.321796071094481e-05,
      "loss": 0.662,
      "step": 14500
    },
    {
      "epoch": 14.03180542563143,
      "grad_norm": 0.9497776031494141,
      "learning_rate": 4.298409728718429e-05,
      "loss": 0.6624,
      "step": 15000
    },
    {
      "epoch": 14.499532273152479,
      "grad_norm": 1.049006462097168,
      "learning_rate": 4.275023386342376e-05,
      "loss": 0.6601,
      "step": 15500
    },
    {
      "epoch": 14.967259120673527,
      "grad_norm": 1.3215961456298828,
      "learning_rate": 4.251637043966324e-05,
      "loss": 0.6605,
      "step": 16000
    },
    {
      "epoch": 15.434985968194574,
      "grad_norm": 0.897559404373169,
      "learning_rate": 4.228250701590272e-05,
      "loss": 0.6583,
      "step": 16500
    },
    {
      "epoch": 15.902712815715622,
      "grad_norm": 0.9747930765151978,
      "learning_rate": 4.204864359214219e-05,
      "loss": 0.6594,
      "step": 17000
    },
    {
      "epoch": 16.37043966323667,
      "grad_norm": 1.1781580448150635,
      "learning_rate": 4.181478016838167e-05,
      "loss": 0.6571,
      "step": 17500
    },
    {
      "epoch": 16.838166510757716,
      "grad_norm": 0.9199815392494202,
      "learning_rate": 4.1580916744621145e-05,
      "loss": 0.6582,
      "step": 18000
    },
    {
      "epoch": 17.305893358278766,
      "grad_norm": 1.3561264276504517,
      "learning_rate": 4.134705332086062e-05,
      "loss": 0.6559,
      "step": 18500
    },
    {
      "epoch": 17.773620205799812,
      "grad_norm": 0.7840412259101868,
      "learning_rate": 4.11131898971001e-05,
      "loss": 0.6567,
      "step": 19000
    },
    {
      "epoch": 18.24134705332086,
      "grad_norm": 0.7722330689430237,
      "learning_rate": 4.087932647333957e-05,
      "loss": 0.6542,
      "step": 19500
    },
    {
      "epoch": 18.709073900841908,
      "grad_norm": 1.1479183435440063,
      "learning_rate": 4.064546304957905e-05,
      "loss": 0.655,
      "step": 20000
    },
    {
      "epoch": 19.176800748362957,
      "grad_norm": 1.107174277305603,
      "learning_rate": 4.0411599625818524e-05,
      "loss": 0.6549,
      "step": 20500
    },
    {
      "epoch": 19.644527595884004,
      "grad_norm": 0.806843638420105,
      "learning_rate": 4.0177736202058e-05,
      "loss": 0.6526,
      "step": 21000
    },
    {
      "epoch": 20.112254443405053,
      "grad_norm": 0.7269644141197205,
      "learning_rate": 3.9943872778297476e-05,
      "loss": 0.6537,
      "step": 21500
    },
    {
      "epoch": 20.5799812909261,
      "grad_norm": 1.03135347366333,
      "learning_rate": 3.971000935453695e-05,
      "loss": 0.6514,
      "step": 22000
    },
    {
      "epoch": 21.047708138447145,
      "grad_norm": 1.0743470191955566,
      "learning_rate": 3.947614593077643e-05,
      "loss": 0.6535,
      "step": 22500
    },
    {
      "epoch": 21.515434985968195,
      "grad_norm": 0.7641396522521973,
      "learning_rate": 3.9242282507015904e-05,
      "loss": 0.651,
      "step": 23000
    },
    {
      "epoch": 21.98316183348924,
      "grad_norm": 0.857388973236084,
      "learning_rate": 3.900841908325538e-05,
      "loss": 0.6519,
      "step": 23500
    },
    {
      "epoch": 22.45088868101029,
      "grad_norm": 0.9994105100631714,
      "learning_rate": 3.8774555659494855e-05,
      "loss": 0.6497,
      "step": 24000
    },
    {
      "epoch": 22.918615528531337,
      "grad_norm": 1.0847715139389038,
      "learning_rate": 3.854069223573433e-05,
      "loss": 0.6511,
      "step": 24500
    },
    {
      "epoch": 23.386342376052387,
      "grad_norm": 0.8348449468612671,
      "learning_rate": 3.8306828811973814e-05,
      "loss": 0.6489,
      "step": 25000
    },
    {
      "epoch": 23.854069223573433,
      "grad_norm": 0.9161397814750671,
      "learning_rate": 3.807296538821328e-05,
      "loss": 0.6503,
      "step": 25500
    },
    {
      "epoch": 24.321796071094482,
      "grad_norm": 1.1919169425964355,
      "learning_rate": 3.783910196445276e-05,
      "loss": 0.648,
      "step": 26000
    },
    {
      "epoch": 24.78952291861553,
      "grad_norm": 0.7399413585662842,
      "learning_rate": 3.760523854069224e-05,
      "loss": 0.6486,
      "step": 26500
    },
    {
      "epoch": 25.257249766136574,
      "grad_norm": 0.8185005784034729,
      "learning_rate": 3.737137511693171e-05,
      "loss": 0.6471,
      "step": 27000
    },
    {
      "epoch": 25.724976613657624,
      "grad_norm": 0.7753197550773621,
      "learning_rate": 3.713751169317119e-05,
      "loss": 0.648,
      "step": 27500
    },
    {
      "epoch": 26.19270346117867,
      "grad_norm": 0.5769951939582825,
      "learning_rate": 3.690364826941067e-05,
      "loss": 0.6478,
      "step": 28000
    },
    {
      "epoch": 26.66043030869972,
      "grad_norm": 0.6810651421546936,
      "learning_rate": 3.666978484565014e-05,
      "loss": 0.647,
      "step": 28500
    },
    {
      "epoch": 27.128157156220766,
      "grad_norm": 0.8123717308044434,
      "learning_rate": 3.643592142188962e-05,
      "loss": 0.6461,
      "step": 29000
    },
    {
      "epoch": 27.595884003741816,
      "grad_norm": 0.9796727299690247,
      "learning_rate": 3.6202057998129096e-05,
      "loss": 0.6454,
      "step": 29500
    },
    {
      "epoch": 28.06361085126286,
      "grad_norm": 1.0182992219924927,
      "learning_rate": 3.596819457436857e-05,
      "loss": 0.6456,
      "step": 30000
    },
    {
      "epoch": 28.53133769878391,
      "grad_norm": 1.018452763557434,
      "learning_rate": 3.573433115060805e-05,
      "loss": 0.6439,
      "step": 30500
    },
    {
      "epoch": 28.999064546304957,
      "grad_norm": 0.6222241520881653,
      "learning_rate": 3.5500467726847523e-05,
      "loss": 0.6461,
      "step": 31000
    },
    {
      "epoch": 29.466791393826007,
      "grad_norm": 0.8786205649375916,
      "learning_rate": 3.5266604303087e-05,
      "loss": 0.6437,
      "step": 31500
    },
    {
      "epoch": 29.934518241347053,
      "grad_norm": 0.6732379198074341,
      "learning_rate": 3.5032740879326475e-05,
      "loss": 0.6445,
      "step": 32000
    },
    {
      "epoch": 30.4022450888681,
      "grad_norm": 0.9794049859046936,
      "learning_rate": 3.479887745556595e-05,
      "loss": 0.6429,
      "step": 32500
    },
    {
      "epoch": 30.86997193638915,
      "grad_norm": 0.6680418848991394,
      "learning_rate": 3.456501403180543e-05,
      "loss": 0.6437,
      "step": 33000
    },
    {
      "epoch": 31.337698783910195,
      "grad_norm": 0.6774255037307739,
      "learning_rate": 3.43311506080449e-05,
      "loss": 0.6424,
      "step": 33500
    },
    {
      "epoch": 31.805425631431245,
      "grad_norm": 0.6334225535392761,
      "learning_rate": 3.4097287184284385e-05,
      "loss": 0.6428,
      "step": 34000
    },
    {
      "epoch": 32.27315247895229,
      "grad_norm": 0.9134377241134644,
      "learning_rate": 3.3863423760523854e-05,
      "loss": 0.6414,
      "step": 34500
    },
    {
      "epoch": 32.74087932647334,
      "grad_norm": 0.7105947732925415,
      "learning_rate": 3.362956033676333e-05,
      "loss": 0.642,
      "step": 35000
    },
    {
      "epoch": 33.20860617399439,
      "grad_norm": 0.8363071084022522,
      "learning_rate": 3.3395696913002806e-05,
      "loss": 0.6404,
      "step": 35500
    },
    {
      "epoch": 33.67633302151543,
      "grad_norm": 0.6887248754501343,
      "learning_rate": 3.316183348924228e-05,
      "loss": 0.6413,
      "step": 36000
    },
    {
      "epoch": 34.14405986903648,
      "grad_norm": 0.7843846082687378,
      "learning_rate": 3.2927970065481764e-05,
      "loss": 0.6402,
      "step": 36500
    },
    {
      "epoch": 34.61178671655753,
      "grad_norm": 0.8591806888580322,
      "learning_rate": 3.269410664172123e-05,
      "loss": 0.6399,
      "step": 37000
    },
    {
      "epoch": 35.07951356407858,
      "grad_norm": 1.0343035459518433,
      "learning_rate": 3.246024321796071e-05,
      "loss": 0.6401,
      "step": 37500
    },
    {
      "epoch": 35.547240411599624,
      "grad_norm": 0.9322572946548462,
      "learning_rate": 3.222637979420019e-05,
      "loss": 0.6387,
      "step": 38000
    },
    {
      "epoch": 36.014967259120674,
      "grad_norm": 0.7423941493034363,
      "learning_rate": 3.199251637043966e-05,
      "loss": 0.6401,
      "step": 38500
    },
    {
      "epoch": 36.48269410664172,
      "grad_norm": 1.0021001100540161,
      "learning_rate": 3.175865294667914e-05,
      "loss": 0.6374,
      "step": 39000
    },
    {
      "epoch": 36.950420954162766,
      "grad_norm": 0.6733675003051758,
      "learning_rate": 3.152478952291862e-05,
      "loss": 0.6393,
      "step": 39500
    },
    {
      "epoch": 37.418147801683816,
      "grad_norm": 0.739268958568573,
      "learning_rate": 3.129092609915809e-05,
      "loss": 0.6374,
      "step": 40000
    },
    {
      "epoch": 37.885874649204865,
      "grad_norm": 0.6018526554107666,
      "learning_rate": 3.105706267539757e-05,
      "loss": 0.6382,
      "step": 40500
    },
    {
      "epoch": 38.353601496725915,
      "grad_norm": 0.6615357995033264,
      "learning_rate": 3.0823199251637047e-05,
      "loss": 0.6359,
      "step": 41000
    },
    {
      "epoch": 38.82132834424696,
      "grad_norm": 0.5983477234840393,
      "learning_rate": 3.058933582787652e-05,
      "loss": 0.637,
      "step": 41500
    },
    {
      "epoch": 39.28905519176801,
      "grad_norm": 0.6511589884757996,
      "learning_rate": 3.0355472404115998e-05,
      "loss": 0.6357,
      "step": 42000
    },
    {
      "epoch": 39.75678203928906,
      "grad_norm": 0.7927898168563843,
      "learning_rate": 3.0121608980355477e-05,
      "loss": 0.636,
      "step": 42500
    },
    {
      "epoch": 40.224508886810106,
      "grad_norm": 0.6924242973327637,
      "learning_rate": 2.988774555659495e-05,
      "loss": 0.6351,
      "step": 43000
    },
    {
      "epoch": 40.69223573433115,
      "grad_norm": 0.6805000305175781,
      "learning_rate": 2.9653882132834426e-05,
      "loss": 0.6347,
      "step": 43500
    },
    {
      "epoch": 41.1599625818522,
      "grad_norm": 0.6107676029205322,
      "learning_rate": 2.9420018709073905e-05,
      "loss": 0.6345,
      "step": 44000
    },
    {
      "epoch": 41.62768942937325,
      "grad_norm": 0.5993724465370178,
      "learning_rate": 2.9186155285313377e-05,
      "loss": 0.6341,
      "step": 44500
    },
    {
      "epoch": 42.09541627689429,
      "grad_norm": 0.5092853307723999,
      "learning_rate": 2.8952291861552856e-05,
      "loss": 0.6346,
      "step": 45000
    },
    {
      "epoch": 42.56314312441534,
      "grad_norm": 0.7202319502830505,
      "learning_rate": 2.8718428437792332e-05,
      "loss": 0.6333,
      "step": 45500
    },
    {
      "epoch": 43.03086997193639,
      "grad_norm": 0.8541982769966125,
      "learning_rate": 2.8484565014031805e-05,
      "loss": 0.6338,
      "step": 46000
    },
    {
      "epoch": 43.49859681945744,
      "grad_norm": 0.8512479662895203,
      "learning_rate": 2.8250701590271284e-05,
      "loss": 0.6315,
      "step": 46500
    },
    {
      "epoch": 43.96632366697848,
      "grad_norm": 0.7734328508377075,
      "learning_rate": 2.8016838166510763e-05,
      "loss": 0.6332,
      "step": 47000
    },
    {
      "epoch": 44.43405051449953,
      "grad_norm": 0.5935522317886353,
      "learning_rate": 2.7782974742750236e-05,
      "loss": 0.6301,
      "step": 47500
    },
    {
      "epoch": 44.90177736202058,
      "grad_norm": 0.5525399446487427,
      "learning_rate": 2.754911131898971e-05,
      "loss": 0.6322,
      "step": 48000
    },
    {
      "epoch": 45.36950420954163,
      "grad_norm": 0.5367240905761719,
      "learning_rate": 2.731524789522919e-05,
      "loss": 0.6307,
      "step": 48500
    },
    {
      "epoch": 45.837231057062674,
      "grad_norm": 0.5478251576423645,
      "learning_rate": 2.7081384471468663e-05,
      "loss": 0.6314,
      "step": 49000
    },
    {
      "epoch": 46.30495790458372,
      "grad_norm": 0.6459486484527588,
      "learning_rate": 2.6847521047708142e-05,
      "loss": 0.6298,
      "step": 49500
    },
    {
      "epoch": 46.77268475210477,
      "grad_norm": 0.7537509202957153,
      "learning_rate": 2.6613657623947615e-05,
      "loss": 0.6306,
      "step": 50000
    },
    {
      "epoch": 47.240411599625816,
      "grad_norm": 0.6679614782333374,
      "learning_rate": 2.637979420018709e-05,
      "loss": 0.6296,
      "step": 50500
    },
    {
      "epoch": 47.708138447146865,
      "grad_norm": 0.6266001462936401,
      "learning_rate": 2.614593077642657e-05,
      "loss": 0.6303,
      "step": 51000
    },
    {
      "epoch": 48.175865294667915,
      "grad_norm": 0.8602786064147949,
      "learning_rate": 2.5912067352666042e-05,
      "loss": 0.629,
      "step": 51500
    },
    {
      "epoch": 48.643592142188965,
      "grad_norm": 0.5469025373458862,
      "learning_rate": 2.567820392890552e-05,
      "loss": 0.6289,
      "step": 52000
    },
    {
      "epoch": 49.11131898971001,
      "grad_norm": 0.643727719783783,
      "learning_rate": 2.5444340505144997e-05,
      "loss": 0.629,
      "step": 52500
    },
    {
      "epoch": 49.57904583723106,
      "grad_norm": 0.7538070678710938,
      "learning_rate": 2.521047708138447e-05,
      "loss": 0.628,
      "step": 53000
    },
    {
      "epoch": 50.046772684752106,
      "grad_norm": 0.6462219953536987,
      "learning_rate": 2.497661365762395e-05,
      "loss": 0.6283,
      "step": 53500
    },
    {
      "epoch": 50.51449953227315,
      "grad_norm": 0.7327728867530823,
      "learning_rate": 2.4742750233863425e-05,
      "loss": 0.6274,
      "step": 54000
    },
    {
      "epoch": 50.9822263797942,
      "grad_norm": 0.8208141326904297,
      "learning_rate": 2.45088868101029e-05,
      "loss": 0.6281,
      "step": 54500
    },
    {
      "epoch": 51.44995322731525,
      "grad_norm": 0.8608967065811157,
      "learning_rate": 2.4275023386342376e-05,
      "loss": 0.6258,
      "step": 55000
    },
    {
      "epoch": 51.9176800748363,
      "grad_norm": 0.5853164792060852,
      "learning_rate": 2.4041159962581852e-05,
      "loss": 0.6276,
      "step": 55500
    },
    {
      "epoch": 52.38540692235734,
      "grad_norm": 0.5878245234489441,
      "learning_rate": 2.380729653882133e-05,
      "loss": 0.6252,
      "step": 56000
    },
    {
      "epoch": 52.85313376987839,
      "grad_norm": 0.624836802482605,
      "learning_rate": 2.3573433115060807e-05,
      "loss": 0.6273,
      "step": 56500
    },
    {
      "epoch": 53.32086061739944,
      "grad_norm": 0.6509925127029419,
      "learning_rate": 2.333956969130028e-05,
      "loss": 0.6258,
      "step": 57000
    },
    {
      "epoch": 53.78858746492049,
      "grad_norm": 0.7883061766624451,
      "learning_rate": 2.310570626753976e-05,
      "loss": 0.6261,
      "step": 57500
    },
    {
      "epoch": 54.25631431244153,
      "grad_norm": 0.6352756023406982,
      "learning_rate": 2.2871842843779235e-05,
      "loss": 0.6249,
      "step": 58000
    },
    {
      "epoch": 54.72404115996258,
      "grad_norm": 0.8056511282920837,
      "learning_rate": 2.263797942001871e-05,
      "loss": 0.6246,
      "step": 58500
    },
    {
      "epoch": 55.19176800748363,
      "grad_norm": 0.9705493450164795,
      "learning_rate": 2.2404115996258186e-05,
      "loss": 0.6244,
      "step": 59000
    },
    {
      "epoch": 55.65949485500468,
      "grad_norm": 0.6868740320205688,
      "learning_rate": 2.2170252572497662e-05,
      "loss": 0.6241,
      "step": 59500
    },
    {
      "epoch": 56.12722170252572,
      "grad_norm": 0.6025754809379578,
      "learning_rate": 2.1936389148737138e-05,
      "loss": 0.6244,
      "step": 60000
    },
    {
      "epoch": 56.59494855004677,
      "grad_norm": 0.7696021199226379,
      "learning_rate": 2.1702525724976614e-05,
      "loss": 0.6238,
      "step": 60500
    },
    {
      "epoch": 57.06267539756782,
      "grad_norm": 0.8768881559371948,
      "learning_rate": 2.1468662301216093e-05,
      "loss": 0.6245,
      "step": 61000
    },
    {
      "epoch": 57.530402245088865,
      "grad_norm": 0.699455201625824,
      "learning_rate": 2.1234798877455565e-05,
      "loss": 0.6227,
      "step": 61500
    },
    {
      "epoch": 57.998129092609915,
      "grad_norm": 0.8353672623634338,
      "learning_rate": 2.100093545369504e-05,
      "loss": 0.6237,
      "step": 62000
    },
    {
      "epoch": 58.465855940130965,
      "grad_norm": 0.6234889626502991,
      "learning_rate": 2.076707202993452e-05,
      "loss": 0.6215,
      "step": 62500
    },
    {
      "epoch": 58.933582787652014,
      "grad_norm": 0.7150460481643677,
      "learning_rate": 2.0533208606173996e-05,
      "loss": 0.6232,
      "step": 63000
    },
    {
      "epoch": 59.40130963517306,
      "grad_norm": 0.7461994886398315,
      "learning_rate": 2.0299345182413472e-05,
      "loss": 0.6218,
      "step": 63500
    },
    {
      "epoch": 59.869036482694106,
      "grad_norm": 0.834999144077301,
      "learning_rate": 2.0065481758652948e-05,
      "loss": 0.6228,
      "step": 64000
    },
    {
      "epoch": 60.336763330215156,
      "grad_norm": 0.5596509575843811,
      "learning_rate": 1.9831618334892424e-05,
      "loss": 0.6203,
      "step": 64500
    },
    {
      "epoch": 60.8044901777362,
      "grad_norm": 0.8573960065841675,
      "learning_rate": 1.95977549111319e-05,
      "loss": 0.6215,
      "step": 65000
    },
    {
      "epoch": 61.27221702525725,
      "grad_norm": 0.6956213116645813,
      "learning_rate": 1.936389148737138e-05,
      "loss": 0.6211,
      "step": 65500
    },
    {
      "epoch": 61.7399438727783,
      "grad_norm": 0.6303369998931885,
      "learning_rate": 1.913002806361085e-05,
      "loss": 0.6212,
      "step": 66000
    },
    {
      "epoch": 62.20767072029935,
      "grad_norm": 0.8577518463134766,
      "learning_rate": 1.8896164639850327e-05,
      "loss": 0.6204,
      "step": 66500
    },
    {
      "epoch": 62.67539756782039,
      "grad_norm": 0.7832072973251343,
      "learning_rate": 1.8662301216089803e-05,
      "loss": 0.6204,
      "step": 67000
    },
    {
      "epoch": 63.14312441534144,
      "grad_norm": 0.552259624004364,
      "learning_rate": 1.8428437792329282e-05,
      "loss": 0.6198,
      "step": 67500
    },
    {
      "epoch": 63.61085126286249,
      "grad_norm": 0.8859350085258484,
      "learning_rate": 1.8194574368568758e-05,
      "loss": 0.6196,
      "step": 68000
    },
    {
      "epoch": 64.07857811038353,
      "grad_norm": 0.667511522769928,
      "learning_rate": 1.796071094480823e-05,
      "loss": 0.6191,
      "step": 68500
    },
    {
      "epoch": 64.54630495790458,
      "grad_norm": 0.8192451596260071,
      "learning_rate": 1.772684752104771e-05,
      "loss": 0.619,
      "step": 69000
    },
    {
      "epoch": 65.01403180542563,
      "grad_norm": 0.6570842266082764,
      "learning_rate": 1.7492984097287185e-05,
      "loss": 0.6201,
      "step": 69500
    },
    {
      "epoch": 65.48175865294668,
      "grad_norm": 0.7787773013114929,
      "learning_rate": 1.725912067352666e-05,
      "loss": 0.6177,
      "step": 70000
    },
    {
      "epoch": 65.94948550046773,
      "grad_norm": 0.8009717464447021,
      "learning_rate": 1.7025257249766137e-05,
      "loss": 0.6191,
      "step": 70500
    },
    {
      "epoch": 66.41721234798878,
      "grad_norm": 0.6285611987113953,
      "learning_rate": 1.6791393826005613e-05,
      "loss": 0.6176,
      "step": 71000
    },
    {
      "epoch": 66.88493919550982,
      "grad_norm": 0.689509928226471,
      "learning_rate": 1.655753040224509e-05,
      "loss": 0.6185,
      "step": 71500
    },
    {
      "epoch": 67.35266604303087,
      "grad_norm": 0.7743669748306274,
      "learning_rate": 1.6323666978484568e-05,
      "loss": 0.6169,
      "step": 72000
    },
    {
      "epoch": 67.82039289055191,
      "grad_norm": 0.605642557144165,
      "learning_rate": 1.6089803554724043e-05,
      "loss": 0.6179,
      "step": 72500
    },
    {
      "epoch": 68.28811973807296,
      "grad_norm": 0.7447288036346436,
      "learning_rate": 1.5855940130963516e-05,
      "loss": 0.617,
      "step": 73000
    },
    {
      "epoch": 68.75584658559401,
      "grad_norm": 0.5727553367614746,
      "learning_rate": 1.5622076707202995e-05,
      "loss": 0.6174,
      "step": 73500
    },
    {
      "epoch": 69.22357343311506,
      "grad_norm": 0.7765412926673889,
      "learning_rate": 1.538821328344247e-05,
      "loss": 0.6167,
      "step": 74000
    },
    {
      "epoch": 69.69130028063611,
      "grad_norm": 0.7251763343811035,
      "learning_rate": 1.5154349859681947e-05,
      "loss": 0.6167,
      "step": 74500
    },
    {
      "epoch": 70.15902712815716,
      "grad_norm": 0.5961426496505737,
      "learning_rate": 1.492048643592142e-05,
      "loss": 0.6167,
      "step": 75000
    },
    {
      "epoch": 70.6267539756782,
      "grad_norm": 0.848954975605011,
      "learning_rate": 1.46866230121609e-05,
      "loss": 0.6166,
      "step": 75500
    },
    {
      "epoch": 71.09448082319925,
      "grad_norm": 0.8628749251365662,
      "learning_rate": 1.4452759588400374e-05,
      "loss": 0.616,
      "step": 76000
    },
    {
      "epoch": 71.5622076707203,
      "grad_norm": 0.699506402015686,
      "learning_rate": 1.421889616463985e-05,
      "loss": 0.6153,
      "step": 76500
    },
    {
      "epoch": 72.02993451824135,
      "grad_norm": 0.657245397567749,
      "learning_rate": 1.3985032740879327e-05,
      "loss": 0.6162,
      "step": 77000
    },
    {
      "epoch": 72.4976613657624,
      "grad_norm": 0.6907195448875427,
      "learning_rate": 1.3751169317118803e-05,
      "loss": 0.6149,
      "step": 77500
    },
    {
      "epoch": 72.96538821328345,
      "grad_norm": 0.7171382308006287,
      "learning_rate": 1.3517305893358279e-05,
      "loss": 0.6157,
      "step": 78000
    },
    {
      "epoch": 73.4331150608045,
      "grad_norm": 0.6256409883499146,
      "learning_rate": 1.3283442469597757e-05,
      "loss": 0.6138,
      "step": 78500
    },
    {
      "epoch": 73.90084190832553,
      "grad_norm": 0.6699703931808472,
      "learning_rate": 1.3049579045837232e-05,
      "loss": 0.616,
      "step": 79000
    },
    {
      "epoch": 74.36856875584658,
      "grad_norm": 0.8775612115859985,
      "learning_rate": 1.2815715622076707e-05,
      "loss": 0.6144,
      "step": 79500
    },
    {
      "epoch": 74.83629560336763,
      "grad_norm": 0.8639257550239563,
      "learning_rate": 1.2581852198316186e-05,
      "loss": 0.6145,
      "step": 80000
    },
    {
      "epoch": 75.30402245088868,
      "grad_norm": 0.8829948902130127,
      "learning_rate": 1.234798877455566e-05,
      "loss": 0.6133,
      "step": 80500
    },
    {
      "epoch": 75.77174929840973,
      "grad_norm": 0.5430837869644165,
      "learning_rate": 1.2114125350795136e-05,
      "loss": 0.6137,
      "step": 81000
    },
    {
      "epoch": 76.23947614593078,
      "grad_norm": 0.8835317492485046,
      "learning_rate": 1.1880261927034613e-05,
      "loss": 0.6137,
      "step": 81500
    },
    {
      "epoch": 76.70720299345183,
      "grad_norm": 0.7837302088737488,
      "learning_rate": 1.1646398503274089e-05,
      "loss": 0.614,
      "step": 82000
    },
    {
      "epoch": 77.17492984097287,
      "grad_norm": 0.7538653612136841,
      "learning_rate": 1.1412535079513565e-05,
      "loss": 0.6136,
      "step": 82500
    },
    {
      "epoch": 77.64265668849391,
      "grad_norm": 0.7095761299133301,
      "learning_rate": 1.117867165575304e-05,
      "loss": 0.6125,
      "step": 83000
    },
    {
      "epoch": 78.11038353601496,
      "grad_norm": 0.6511927843093872,
      "learning_rate": 1.0944808231992516e-05,
      "loss": 0.6132,
      "step": 83500
    },
    {
      "epoch": 78.57811038353601,
      "grad_norm": 0.8284214735031128,
      "learning_rate": 1.0710944808231992e-05,
      "loss": 0.6126,
      "step": 84000
    },
    {
      "epoch": 79.04583723105706,
      "grad_norm": 0.5522418022155762,
      "learning_rate": 1.047708138447147e-05,
      "loss": 0.6132,
      "step": 84500
    },
    {
      "epoch": 79.51356407857811,
      "grad_norm": 0.6696499586105347,
      "learning_rate": 1.0243217960710946e-05,
      "loss": 0.6119,
      "step": 85000
    },
    {
      "epoch": 79.98129092609916,
      "grad_norm": 0.6263949275016785,
      "learning_rate": 1.0009354536950421e-05,
      "loss": 0.6126,
      "step": 85500
    },
    {
      "epoch": 80.44901777362021,
      "grad_norm": 0.7786533236503601,
      "learning_rate": 9.775491113189899e-06,
      "loss": 0.6123,
      "step": 86000
    },
    {
      "epoch": 80.91674462114125,
      "grad_norm": 0.8267824053764343,
      "learning_rate": 9.541627689429373e-06,
      "loss": 0.612,
      "step": 86500
    },
    {
      "epoch": 81.3844714686623,
      "grad_norm": 0.8671319484710693,
      "learning_rate": 9.30776426566885e-06,
      "loss": 0.611,
      "step": 87000
    },
    {
      "epoch": 81.85219831618335,
      "grad_norm": 0.5700300335884094,
      "learning_rate": 9.073900841908325e-06,
      "loss": 0.6115,
      "step": 87500
    },
    {
      "epoch": 82.3199251637044,
      "grad_norm": 0.826460599899292,
      "learning_rate": 8.840037418147802e-06,
      "loss": 0.6115,
      "step": 88000
    },
    {
      "epoch": 82.78765201122545,
      "grad_norm": 0.702474057674408,
      "learning_rate": 8.606173994387278e-06,
      "loss": 0.6112,
      "step": 88500
    },
    {
      "epoch": 83.2553788587465,
      "grad_norm": 0.8015270829200745,
      "learning_rate": 8.372310570626754e-06,
      "loss": 0.6116,
      "step": 89000
    },
    {
      "epoch": 83.72310570626755,
      "grad_norm": 0.7060162425041199,
      "learning_rate": 8.138447146866231e-06,
      "loss": 0.6105,
      "step": 89500
    },
    {
      "epoch": 84.19083255378858,
      "grad_norm": 0.6131981015205383,
      "learning_rate": 7.904583723105706e-06,
      "loss": 0.6111,
      "step": 90000
    },
    {
      "epoch": 84.65855940130963,
      "grad_norm": 0.7261953949928284,
      "learning_rate": 7.670720299345183e-06,
      "loss": 0.6102,
      "step": 90500
    },
    {
      "epoch": 85.12628624883068,
      "grad_norm": 0.7434335350990295,
      "learning_rate": 7.43685687558466e-06,
      "loss": 0.6106,
      "step": 91000
    },
    {
      "epoch": 85.59401309635173,
      "grad_norm": 0.6174488663673401,
      "learning_rate": 7.202993451824135e-06,
      "loss": 0.6107,
      "step": 91500
    },
    {
      "epoch": 86.06173994387278,
      "grad_norm": 0.6684290170669556,
      "learning_rate": 6.969130028063611e-06,
      "loss": 0.6101,
      "step": 92000
    },
    {
      "epoch": 86.52946679139383,
      "grad_norm": 0.8239431381225586,
      "learning_rate": 6.735266604303088e-06,
      "loss": 0.6104,
      "step": 92500
    },
    {
      "epoch": 86.99719363891488,
      "grad_norm": 0.7832866311073303,
      "learning_rate": 6.501403180542563e-06,
      "loss": 0.6108,
      "step": 93000
    },
    {
      "epoch": 87.46492048643591,
      "grad_norm": 0.5820294618606567,
      "learning_rate": 6.26753975678204e-06,
      "loss": 0.6091,
      "step": 93500
    },
    {
      "epoch": 87.93264733395696,
      "grad_norm": 0.7305190563201904,
      "learning_rate": 6.0336763330215154e-06,
      "loss": 0.6101,
      "step": 94000
    },
    {
      "epoch": 88.40037418147801,
      "grad_norm": 0.6824904084205627,
      "learning_rate": 5.799812909260992e-06,
      "loss": 0.6091,
      "step": 94500
    },
    {
      "epoch": 88.86810102899906,
      "grad_norm": 0.6762772798538208,
      "learning_rate": 5.565949485500468e-06,
      "loss": 0.6107,
      "step": 95000
    },
    {
      "epoch": 89.33582787652011,
      "grad_norm": 0.6610039472579956,
      "learning_rate": 5.332086061739945e-06,
      "loss": 0.6085,
      "step": 95500
    },
    {
      "epoch": 89.80355472404116,
      "grad_norm": 0.7032989263534546,
      "learning_rate": 5.09822263797942e-06,
      "loss": 0.6091,
      "step": 96000
    },
    {
      "epoch": 90.27128157156221,
      "grad_norm": 0.7655551433563232,
      "learning_rate": 4.864359214218896e-06,
      "loss": 0.6094,
      "step": 96500
    },
    {
      "epoch": 90.73900841908326,
      "grad_norm": 0.6718347072601318,
      "learning_rate": 4.630495790458372e-06,
      "loss": 0.6091,
      "step": 97000
    },
    {
      "epoch": 91.2067352666043,
      "grad_norm": 0.9554619789123535,
      "learning_rate": 4.396632366697849e-06,
      "loss": 0.6091,
      "step": 97500
    },
    {
      "epoch": 91.67446211412535,
      "grad_norm": 0.8019670844078064,
      "learning_rate": 4.162768942937325e-06,
      "loss": 0.6084,
      "step": 98000
    },
    {
      "epoch": 92.1421889616464,
      "grad_norm": 0.6838127374649048,
      "learning_rate": 3.928905519176801e-06,
      "loss": 0.609,
      "step": 98500
    },
    {
      "epoch": 92.60991580916745,
      "grad_norm": 0.6558223366737366,
      "learning_rate": 3.695042095416277e-06,
      "loss": 0.6084,
      "step": 99000
    },
    {
      "epoch": 93.0776426566885,
      "grad_norm": 0.5459709167480469,
      "learning_rate": 3.461178671655753e-06,
      "loss": 0.6086,
      "step": 99500
    },
    {
      "epoch": 93.54536950420955,
      "grad_norm": 0.9122899174690247,
      "learning_rate": 3.2273152478952295e-06,
      "loss": 0.608,
      "step": 100000
    },
    {
      "epoch": 94.0130963517306,
      "grad_norm": 0.7761320471763611,
      "learning_rate": 2.9934518241347057e-06,
      "loss": 0.6084,
      "step": 100500
    },
    {
      "epoch": 94.48082319925163,
      "grad_norm": 0.7342089414596558,
      "learning_rate": 2.7595884003741815e-06,
      "loss": 0.608,
      "step": 101000
    },
    {
      "epoch": 94.94855004677268,
      "grad_norm": 0.8833499550819397,
      "learning_rate": 2.5257249766136578e-06,
      "loss": 0.6079,
      "step": 101500
    },
    {
      "epoch": 95.41627689429373,
      "grad_norm": 0.7534879446029663,
      "learning_rate": 2.2918615528531336e-06,
      "loss": 0.6079,
      "step": 102000
    },
    {
      "epoch": 95.88400374181478,
      "grad_norm": 0.6253385543823242,
      "learning_rate": 2.0579981290926103e-06,
      "loss": 0.6078,
      "step": 102500
    },
    {
      "epoch": 96.35173058933583,
      "grad_norm": 0.760979175567627,
      "learning_rate": 1.824134705332086e-06,
      "loss": 0.6083,
      "step": 103000
    },
    {
      "epoch": 96.81945743685688,
      "grad_norm": 0.711215078830719,
      "learning_rate": 1.5902712815715623e-06,
      "loss": 0.6074,
      "step": 103500
    },
    {
      "epoch": 97.28718428437793,
      "grad_norm": 0.7351893782615662,
      "learning_rate": 1.3564078578110384e-06,
      "loss": 0.6068,
      "step": 104000
    },
    {
      "epoch": 97.75491113189896,
      "grad_norm": 0.6618276834487915,
      "learning_rate": 1.1225444340505144e-06,
      "loss": 0.6084,
      "step": 104500
    },
    {
      "epoch": 98.22263797942001,
      "grad_norm": 0.6330026388168335,
      "learning_rate": 8.886810102899906e-07,
      "loss": 0.6081,
      "step": 105000
    },
    {
      "epoch": 98.69036482694106,
      "grad_norm": 0.6944205164909363,
      "learning_rate": 6.548175865294669e-07,
      "loss": 0.6073,
      "step": 105500
    },
    {
      "epoch": 99.15809167446211,
      "grad_norm": 0.7151799201965332,
      "learning_rate": 4.20954162768943e-07,
      "loss": 0.608,
      "step": 106000
    },
    {
      "epoch": 99.62581852198316,
      "grad_norm": 0.7164794206619263,
      "learning_rate": 1.870907390084191e-07,
      "loss": 0.6073,
      "step": 106500
    }
  ],
  "logging_steps": 500,
  "max_steps": 106900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 100,
  "save_steps": 400000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.40163132231936e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
